# AUTOGENERATED! DO NOT EDIT! File to edit: 03_Get_Embedding.ipynb (unless otherwise specified).

__all__ = ['batch_converter', 'model', 'getSequenceRepresentation']

# Cell
import torch
import esm

from tqdm.notebook import tqdm,trange

# Cell
# Load ESM-1b model
model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()
batch_converter = alphabet.get_batch_converter()

model = model.to("cuda:1")

# Cell
def getSequenceRepresentation(Data):
    """
    Embed the given variants using the ESM-1b model

    Arguments:
    - Data : list[(id, sequence),]
    """
    batch_labels, batch_strs, batch_tokens = batch_converter(Data)
    batch_tokens = batch_tokens.to("cuda:1")
    # Extract per-residue representations (on CPU)
    with torch.no_grad():
        results = model(batch_tokens, repr_layers=[33], return_contacts=True)
    token_representations = results["representations"][33].cpu()
    del results, batch_labels, batch_strs, batch_tokens
    # Generate per-sequence representations via averaging
    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.
    sequence_representations = []
    for i, (_, seq) in enumerate(Data):
        sequence_representations.append(token_representations[i, 1 : len(seq) + 1].cpu().numpy())
    del token_representations
    return sequence_representations#, results