{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7127cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fair-esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb115eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90930924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Load ESM-1b model\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9103b",
   "metadata": {},
   "source": [
    "# Example from repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd3eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getSequenceRepresentation(Data):\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(Data)\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, (_, seq) in enumerate(Data):\n",
    "        sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))\n",
    "    return sequence_representations, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "seuence_represntations, results = getSequenceRepresentation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943206ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the unsupervised self-attention map contact predictions\n",
    "import matplotlib.pyplot as plt\n",
    "for (_, seq), attention_contacts in zip(data, results[\"contacts\"]):\n",
    "    plt.matshow(attention_contacts[: len(seq), : len(seq)])\n",
    "    plt.title(seq)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processBioDBs.utilities import getSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c5af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation,results2 = getSequenceRepresentation([(\"FoxRed1\",getSequence(\"FoxRed1\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366d81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
