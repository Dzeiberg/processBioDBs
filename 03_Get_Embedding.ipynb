{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ef81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fair-esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d925ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d7302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Load ESM-1b model\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7f9bb",
   "metadata": {},
   "source": [
    "# Example from repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6a595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getSequenceRepresentation(Data):\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(Data)\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, (_, seq) in enumerate(Data):\n",
    "        sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))\n",
    "    return sequence_representations, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "seuence_represntations, results = getSequenceRepresentation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the unsupervised self-attention map contact predictions\n",
    "import matplotlib.pyplot as plt\n",
    "for (_, seq), attention_contacts in zip(data, results[\"contacts\"]):\n",
    "    plt.matshow(attention_contacts[: len(seq), : len(seq)])\n",
    "    plt.title(seq)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a973b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processBioDBs.utilities import getSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation,results2 = getSequenceRepresentation([(\"FoxRed1\",getSequence(\"FoxRed1\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5d96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
